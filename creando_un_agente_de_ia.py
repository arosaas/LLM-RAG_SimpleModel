# -*- coding: utf-8 -*-
"""Creando un Agente de IA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lj21xpUdpcTbEW9kWC1L-VXjQWMaEOTM

# Reto - Creando un agente RAG

En este taller buscaremos desarrollar un agente RAG que sea capaz de respoder preguntas sencillas de manera precisa, empleando como base de conocimiento el texto extra칤do de distintos enlaces web y los documentos que decidamos compartir.

```
# This is formatted as code
```

## CONFIGURACI칍N DE UN LLM

El LLM que emplearemos para generar las respuestas de nuestro RAG ser치 **Gemini-2.0-Pro**.

Para conseguir una clave de acceso para utilizar los modelos gratuitos de Gemini, tendremos que acceder a [Google AI Studio](https://aistudio.google.com/app/apikey?hl=es-419). Una vez aqu칤, bastar치 con iniciar sesi칩n con nuestra cuenta de Google y posteriormente pulsar en "Crear clave de API". Tras seleccionar un proyecto cualquiera y pulsar en "Crear clave de API en un proyecto existente", obtendremos la clave API que deberemos copiar.

丘멆잺 Es **importante** que copiemos el valor que nos aparecer치 en pantalla ya que, al cerrar esa pantalla, no volveremeos a tener acceso a 칠l (ser칤a necesario volver a crear una clave nueva).

Esta clave de API la guardaremos en la secci칩n de *Secretos* de Google Collab. Para ello, basta pulsar el icono "游댐" que aparece en el lateral izquierdo de nuestro notebook de Google Colab, y pulsar en ***A침adir secreto nuevo***. Una vez aqu칤, colocamos **Nombre: GOOGLE_API_KEY**, y para el **Valor**, la clave que hab칤amos copiado para nuestro token.

Una vez obtenida la clave de API, configuramos el LLM
"""

pip install -qU google-generativeai

import google.generativeai as genai
from google.colab import userdata

GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)

generation_config = {
    "temperature": 0.1,
    "top_p": 0.9,
    "top_k": 64,
    "max_output_tokens": 10000,
    "response_mime_type": "text/plain",
  }

llm = genai.GenerativeModel(
      model_name="gemini-2.0-flash",
      generation_config=generation_config,
  )

"""## Configurando el Generador de Embeddings

Para ello ser치 necesario que generemos un token de Hugging Face.

Si a칰n no tienes una cuenta, puedes registrarte en [Hugging Face](https://huggingface.co/). Solo necesitas proporcionar un correo electr칩nico, una contrase침a y activar tu cuenta a trav칠s del correo de verificaci칩n que recibir치s.  


Una vez registrados, debemos de seguir estos pasos para generar un token:
1. Accede a tu perfil en Hugging Face.  
2. Ve a la secci칩n de **Access Tokens**.  
3. Haz clic en **Create new token**.  
4. Selecciona el nivel de acceso **Read**.  
5. Asigna un nombre a tu token.  
6. Genera el token.

Una vez creado el token, es **importante** que copiemos el valor que nos aparecer치 en pantalla. Al igual que suced칤a en el caso de la clave API para Gemini, tras cerrar esa pantalla no podremos volver a acceder a 칠l (ser칤a necesario volver a crear un token nuevo).


Este valor de token lo guardaremos en la secci칩n de *Secretos* de Google Collab. Para ello, tal y como hicimos anteriormente, basta pulsar el icono "游댐" que aparece en el lateral izquierdo de nuestro notebook de Google Collab, y pulsar en ***A침adir secreto nuevo***. Una vez aqu칤, colocamos **Nombre: HF_TOKEN**, y para el **Valor**, la clave que hab칤amos copiado para nuestro token.

Una vez ya creado el token y almacenado en los secretos de Google Colab, configuramos el Generador de Embeddings
"""

pip install -qU langchain-huggingface

from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

"""### Configurando la colecci칩n

Instalamos las dependencias necesarias y definimos nuestra colecci칩n o *Vector Store*. Para ello empleamos:
- El Generador de Embeddings que acabamos de configurar.
- RecursiveCharacterTextSplitter para realizar el *chunking*.
- El enlace web "[https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/)" como base de conocmiento
"""

pip install -qU langchain-core langchain-text-splitters langchain-community

from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)

# Install package
!pip install --upgrade --quiet llmsherpa

pip install pypdf

"""# Configurando la colecci칩n offline

Para que nuestro RAG pueda "leer" documentos offline ser치 necesario a침adir la siguiente l칤nea de comando, de forma que pueda almacenar los archivos PDF en un repositorio de Google.
"""

# PARA DOCUMENTOS OFFLINE
from google.colab import files

uploaded = files.upload()

import pypdf
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# PARA DOCUMENTOS OFFLINE
loader = PyPDFLoader(
    file_path="S1TDRC.pdf")

# PARA DOCUMENTOS ONLINE
# loader = PyPDFLoader(
#     file_path="https://arxiv.org/pdf/2402.14207.pdf"
# )

# PARA VARIOS DOCUMENTOS OFFLINE
# pdf_files = ["EjercicioT1_Protocolos2025.pdf"]

# docs = []
# for pdf_file in pdf_files:
#     loader = PyPDFLoader(pdf_file)
#     docs.extend(loader.load())

# PARA VARIOS DOCUMENTOS ONLINE
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)

# Index chunks
_ = vector_store.add_documents(documents=all_splits)

"""## Definiendo nuestro agente RAG

Para definir nuestro agente RAG emplearemos LangGraph.
A continuaci칩n, se instalan las dependencias necesarias y se define el prompt que se usar치 para la generaci칩n de texto.
"""

pip install -qU langgraph

rag_prompt = """You are an assistant for question-answering tasks. You have to give an answer based on the following context, make the answers as complete as you can and answer them as if you were not the person asked the question, always give a response in spanish"
Question: {question}
Context: {context}
Answer:"""

"""Una vez hecho esto, en el siguiente c칩digo se define el flujo del agente RAG. En particular, en el c칩digo a continuaci칩n observaremos:
1. **Definici칩n del estado**: Se establece una estructura de datos (`State`) para almacenar la pregunta, el contexto recuperado y la respuesta generada.  
2. **Recuperaci칩n de documentos**: Se busca informaci칩n relevante en la colecci칩n a partir de la pregunta del usuario.  
3. **Generaci칩n de respuesta**: Se construye un prompt con el contexto recuperado y se env칤a al modelo de lenguaje para obtener la respuesta.  
4. **Construcci칩n del grafo**: Se utiliza `StateGraph` para encadenar los pasos de recuperaci칩n y generaci칩n, creando el flujo de procesamiento del agente.  

"""

from langchain_core.documents import Document
from langgraph.graph import START, StateGraph, END
from typing_extensions import List, TypedDict
import numpy as np

# Define state for application
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str

# CALCULA LA SIMILITUD

# def Similarity(state: State):
#     vector_question = embeddings.embed_query(state["question"])
#     # vector_question = numpy.cos(vector_question)
#     # vector_context = embeddings.embed_query(state["context"])
#     vector_context = [embeddings.embed_query(doc.page_content) for doc in state["context"]]
#     # vector_context = numpy.cos(vector_context)
#     # Similarity = numpy.absolute(vector_question - vector_context)
#     vector_context_mean = np.mean(vector_context, axis=0)
#     Similarity = np.dot(vector_question, vector_context_mean) / (np.linalg.norm(vector_question) * np.linalg.norm(vector_context_mean))
#     return {"similarity": Similarity}

# Define application steps
def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"]) # te da el contexto
    # print(f"Retrieved docs: {retrieved_docs} \n")
    return {"context": retrieved_docs}

# EVALUA LA SIMILITUD

# def evaluate(state: State):
#      similarity = Similarity(state)
#      similarity = similarity["similarity"]

#      if similarity < 0.1:
#          return {"next_step":END}
#      if 0.1 <= similarity < 0.8:
#          return{"next_step":"retrieve"}
#      else:
#          return{"next_step":"generate"}


def generate(state: State): # generaci칩n
    global rag_prompt

    docs_content = "\n\n".join(doc.page_content for doc in state["context"]) # unimos los documentos para insertar los prompts
    prompt = rag_prompt
    prompt = prompt.replace("{question}", state["question"]) # reemplazamos pregunta por la que hemos hecho
    prompt = prompt.replace("{context}", docs_content) # reemplazamos por el contexto

    # Pass prompt to the model
    response = llm.generate_content(prompt) # generamos la respuesta con el LLM
    return {"answer": response.text}

# PARTE PARA QUE FUNCIONE CON LOS NODOS SIMPLES

graph_builder = StateGraph(State).add_edge(START,"retrieve")
graph_builder.add_node("retrieve", retrieve)
graph_builder.add_edge("retrieve", "generate")
graph_builder.add_node("generate", generate)
graph_builder.add_edge("generate", END)

graph = graph_builder.compile()
# # Compile application and test
#  ESTO ES PARA QUE HAGA LA COMPROBACI칍N
# graph_builder = StateGraph(State).add_edge(START, "retrieve")
# graph_builder.add_node("retrieve", retrieve)

# graph_builder.add_edge("retrieve", "evaluate")
# graph_builder.add_node("evaluate", evaluate)
# graph_builder.add_node("generate", generate)

# graph_builder.add_edge("evaluate", "retrieve")
# graph_builder.add_edge("evaluate", "generate")
# graph_builder.add_edge("retrieve", END)

# graph = graph_builder.compile()

"""Una vez compilado el grafo, ya podemos realizar consultas a nuestro agente RAG:"""

from IPython.display import Image, display
png = graph.get_graph().draw_mermaid_png()
display(Image(png))

response = graph.invoke({"question": "Buenos d칤as, saluda a nuestros lectores!"}, {"recursion_limit": 100})
print(response["answer"])